{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Entrenamiento, validación y selección\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chofo\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerias que utilizaremos a lo largo del proyecto\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-val-test-split\n",
    "\n",
    "El primer es dar un vistazo a los datos con los que se cuenta. Iniciamos validando el tipo de dato de cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Iniciamos leyendo el archivo de entrada\n",
    "dataset = pd.read_csv(\"data_titanic_proyecto.csv\")\n",
    "\n",
    "# Contamos el número de regisitros y el numero de columnas que tiene el dataset\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId             int64\n",
       "Name                   object\n",
       "Age                   float64\n",
       "SibSp                   int64\n",
       "Parch                   int64\n",
       "Ticket                 object\n",
       "Fare                  float64\n",
       "Cabin                  object\n",
       "Embarked               object\n",
       "passenger_class        object\n",
       "passenger_sex          object\n",
       "passenger_survived     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos el tipo de dato que tiene cada columna en el dataset\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Porcentaje NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>19.865320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>77.104377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0.224467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_class</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_sex</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_survived</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Porcentaje NaN\n",
       "PassengerId               0.000000\n",
       "Name                      0.000000\n",
       "Age                      19.865320\n",
       "SibSp                     0.000000\n",
       "Parch                     0.000000\n",
       "Ticket                    0.000000\n",
       "Fare                      0.000000\n",
       "Cabin                    77.104377\n",
       "Embarked                  0.224467\n",
       "passenger_class           0.000000\n",
       "passenger_sex             0.000000\n",
       "passenger_survived        0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el portancaje de NaN en nuestro dataset\n",
    "pd.DataFrame({'Porcentaje NaN': dataset.isnull().sum() * 100 / len(dataset)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con base al análisis anterior, procedemos a descartar los features de: PassengerId, Name, Ticket y Cabin.\n",
    "\n",
    "La columna 'Cabin' se descarta debido a que posee un 77% de datos NaN los cuales representan una gran incertidumbre. A continuación procedemos a definir 3 variables, una que contendrá las features que no se utilizarán, otra que contendrá las features categoricas y la última el nombre del feature a predecir, en este caso será passenger_survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(ds, useless_features, categorical_features):\n",
    "    \n",
    "    # Eliminamos las features que no se utilizaran del dataset\n",
    "    ds = ds.drop(useless_features, axis=1)\n",
    "    \n",
    "    # Normalizamos las variables que no son categoricas\n",
    "    # Sustituiremos los valores NaN por el valor de la media\n",
    "    imputer = Imputer(strategy=\"median\")\n",
    "    ds_numeric_features = ds.drop(categorical_features, axis=1)\n",
    "    imputer.fit(ds_numeric_features)\n",
    "    ds_imputer = imputer.transform(ds_numeric_features)\n",
    "    ds_numeric_features = pd.DataFrame(ds_imputer, columns=ds_numeric_features.columns)\n",
    "    \n",
    "    # Normalizamos las variables categoricas\n",
    "    ds_categorical_features = ds.drop(ds_numeric_features.columns, axis=1)\n",
    "    ds_categorical_features_encoded = pd.DataFrame()\n",
    "    \n",
    "    # Sustituiremos los valores NaN por 'NaN'\n",
    "    ds_categorical_features = ds_categorical_features.replace(np.nan, 'NaN', regex=True)\n",
    "    \n",
    "    # Usamos LabelEncoder para convertir nuestros valores categoricos a numericos\n",
    "    ds_categorical_features = ds_categorical_features.apply(LabelEncoder().fit_transform)\n",
    "    ds_categorical_features = pd.DataFrame(ds_categorical_features, columns= ds_categorical_features.columns)\n",
    "    \n",
    "    # Concatenamos nuestros dos DataFrames\n",
    "    ds[categorical_features] = ds_categorical_features[categorical_features].values\n",
    "    ds[ds_numeric_features.columns] = ds_numeric_features.values\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age  SibSp  Parch      Fare  Embarked  passenger_class  passenger_sex  \\\n",
      "0    22.0    1.0    0.0    7.2500         3                0              1   \n",
      "1    38.0    1.0    0.0   71.2833         0                2              0   \n",
      "2    26.0    0.0    0.0    7.9250         3                0              0   \n",
      "3    35.0    1.0    0.0   53.1000         3                2              0   \n",
      "4    35.0    0.0    0.0    8.0500         3                0              1   \n",
      "5    28.0    0.0    0.0    8.4583         2                0              1   \n",
      "6    54.0    0.0    0.0   51.8625         3                2              1   \n",
      "7     2.0    3.0    1.0   21.0750         3                0              1   \n",
      "8    27.0    0.0    2.0   11.1333         3                0              0   \n",
      "9    14.0    1.0    0.0   30.0708         0                1              0   \n",
      "10    4.0    1.0    1.0   16.7000         3                0              0   \n",
      "11   58.0    0.0    0.0   26.5500         3                2              0   \n",
      "12   20.0    0.0    0.0    8.0500         3                0              1   \n",
      "13   39.0    1.0    5.0   31.2750         3                0              1   \n",
      "14   14.0    0.0    0.0    7.8542         3                0              0   \n",
      "15   55.0    0.0    0.0   16.0000         3                1              0   \n",
      "16    2.0    4.0    1.0   29.1250         2                0              1   \n",
      "17   28.0    0.0    0.0   13.0000         3                1              1   \n",
      "18   31.0    1.0    0.0   18.0000         3                0              0   \n",
      "19   28.0    0.0    0.0    7.2250         0                0              0   \n",
      "20   35.0    0.0    0.0   26.0000         3                1              1   \n",
      "21   34.0    0.0    0.0   13.0000         3                1              1   \n",
      "22   15.0    0.0    0.0    8.0292         2                0              0   \n",
      "23   28.0    0.0    0.0   35.5000         3                2              1   \n",
      "24    8.0    3.0    1.0   21.0750         3                0              0   \n",
      "25   38.0    1.0    5.0   31.3875         3                0              0   \n",
      "26   28.0    0.0    0.0    7.2250         0                0              1   \n",
      "27   19.0    3.0    2.0  263.0000         3                2              1   \n",
      "28   28.0    0.0    0.0    7.8792         2                0              0   \n",
      "29   28.0    0.0    0.0    7.8958         3                0              1   \n",
      "..    ...    ...    ...       ...       ...              ...            ...   \n",
      "861  21.0    1.0    0.0   11.5000         3                1              1   \n",
      "862  48.0    0.0    0.0   25.9292         3                2              0   \n",
      "863  28.0    8.0    2.0   69.5500         3                0              0   \n",
      "864  24.0    0.0    0.0   13.0000         3                1              1   \n",
      "865  42.0    0.0    0.0   13.0000         3                1              0   \n",
      "866  27.0    1.0    0.0   13.8583         0                1              0   \n",
      "867  31.0    0.0    0.0   50.4958         3                2              1   \n",
      "868  28.0    0.0    0.0    9.5000         3                0              1   \n",
      "869   4.0    1.0    1.0   11.1333         3                0              1   \n",
      "870  26.0    0.0    0.0    7.8958         3                0              1   \n",
      "871  47.0    1.0    1.0   52.5542         3                2              0   \n",
      "872  33.0    0.0    0.0    5.0000         3                2              1   \n",
      "873  47.0    0.0    0.0    9.0000         3                0              1   \n",
      "874  28.0    1.0    0.0   24.0000         0                1              0   \n",
      "875  15.0    0.0    0.0    7.2250         0                0              0   \n",
      "876  20.0    0.0    0.0    9.8458         3                0              1   \n",
      "877  19.0    0.0    0.0    7.8958         3                0              1   \n",
      "878  28.0    0.0    0.0    7.8958         3                0              1   \n",
      "879  56.0    0.0    1.0   83.1583         0                2              0   \n",
      "880  25.0    0.0    1.0   26.0000         3                1              0   \n",
      "881  33.0    0.0    0.0    7.8958         3                0              1   \n",
      "882  22.0    0.0    0.0   10.5167         3                0              0   \n",
      "883  28.0    0.0    0.0   10.5000         3                1              1   \n",
      "884  25.0    0.0    0.0    7.0500         3                0              1   \n",
      "885  39.0    0.0    5.0   29.1250         2                0              0   \n",
      "886  27.0    0.0    0.0   13.0000         3                1              1   \n",
      "887  19.0    0.0    0.0   30.0000         3                2              0   \n",
      "888  28.0    1.0    2.0   23.4500         3                0              0   \n",
      "889  26.0    0.0    0.0   30.0000         0                2              1   \n",
      "890  32.0    0.0    0.0    7.7500         2                0              1   \n",
      "\n",
      "     passenger_survived  \n",
      "0                     0  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                     1  \n",
      "4                     0  \n",
      "5                     0  \n",
      "6                     0  \n",
      "7                     0  \n",
      "8                     1  \n",
      "9                     1  \n",
      "10                    1  \n",
      "11                    1  \n",
      "12                    0  \n",
      "13                    0  \n",
      "14                    0  \n",
      "15                    1  \n",
      "16                    0  \n",
      "17                    1  \n",
      "18                    0  \n",
      "19                    1  \n",
      "20                    0  \n",
      "21                    1  \n",
      "22                    1  \n",
      "23                    1  \n",
      "24                    0  \n",
      "25                    1  \n",
      "26                    0  \n",
      "27                    0  \n",
      "28                    1  \n",
      "29                    0  \n",
      "..                  ...  \n",
      "861                   0  \n",
      "862                   1  \n",
      "863                   0  \n",
      "864                   0  \n",
      "865                   1  \n",
      "866                   1  \n",
      "867                   0  \n",
      "868                   0  \n",
      "869                   1  \n",
      "870                   0  \n",
      "871                   1  \n",
      "872                   0  \n",
      "873                   0  \n",
      "874                   1  \n",
      "875                   1  \n",
      "876                   0  \n",
      "877                   0  \n",
      "878                   0  \n",
      "879                   1  \n",
      "880                   1  \n",
      "881                   0  \n",
      "882                   0  \n",
      "883                   0  \n",
      "884                   0  \n",
      "885                   0  \n",
      "886                   0  \n",
      "887                   1  \n",
      "888                   0  \n",
      "889                   1  \n",
      "890                   0  \n",
      "\n",
      "[891 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chofo\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\utils\\deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos feature engineering para normalizar los datos a una forma que no sea útil\n",
    "useless_features = np.array(['PassengerId','Name','Ticket', 'Cabin'])\n",
    "categorical_features = np.array(['passenger_sex','Embarked','passenger_class','passenger_survived'])\n",
    "normalized_dataset = feature_engineering(dataset, useless_features, categorical_features)\n",
    "\n",
    "print (normalized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego que hemos hecho feature_engineering sobre nuestra data para normalizarla, procedemos a crear nuestros distintos datasets que utilizaremos para entrenar, validar y probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de los datos de entrenamiento:  (605, 7) (605,)\n",
      "Shape de los datos de validación (107, 7) (107,)\n",
      "Shape de los datos de prueba (179, 7) (179,)\n"
     ]
    }
   ],
   "source": [
    "# Hacemos una copia de nuestro dataset normalizado sin incluir la variable a predecir\n",
    "ds_x = normalized_dataset.drop('passenger_survived', axis=1)\n",
    "\n",
    "predict_feature = 'passenger_survived'\n",
    "ds_y = normalized_dataset[predict_feature].copy()\n",
    "\n",
    "# Separamos nuestros datos en datos de prueba y datos de entrenamiento\n",
    "# Utiliazaremos el 80% como datos de entrenamiento\n",
    "x_train, x_test, y_train, y_test = train_test_split(ds_x, ds_y, train_size=0.8)\n",
    "\n",
    "# Utilizamos los datos de entrenamiento para extraer una porción para datos de validación.\n",
    "# Utilizaremos un 20% del dataset de entrenamiento para datos de validación\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.85)\n",
    "\n",
    "print('Shape de los datos de entrenamiento: ', x_train.shape, y_train.shape)\n",
    "print('Shape de los datos de validación', x_validation.shape, y_validation.shape)\n",
    "print('Shape de los datos de prueba', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registro de bitácora\n",
    "\n",
    "Definimos unos métodos que serán de utilidad para registrar la información relevante de cada experimento que se haga con cada uno de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=eWswOZbSoCA\n",
    "def registrar_experimento(df, nombre_archivo):\n",
    "    if not os.path.isfile(nombre_archivo):\n",
    "        # Si el archivo no existe, lo crearmos y agregamos el dataframe con todo y sus encabezados\n",
    "        df.to_csv(nombre_archivo)\n",
    "    else: \n",
    "        # Si el archivo ya existe, agregamos el dataframe sin incluir sus encabezados\n",
    "        df.to_csv(nombre_archivo, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "A lo largo del proyecto se utilizará Ensemble Learning de cuatro distintos modelos obtenidos a partir de:\n",
    "\n",
    "- Árbol de decisión con sklearn\n",
    "- SVM con sklearn\n",
    "- Naive bayes con numpy y/o pandas\n",
    "- Reg. logística binaria(sigmoid) utilizando Tensorflow\n",
    "\n",
    "Para el proyecto actual no se utiliza la técnica de muestreo de bootstraping, sin embargo ......\n",
    "\n",
    "# AGREGAR COMENTARIO DE BOOTSTRAPING\n",
    "\n",
    "Parameter selection\n",
    "https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos función para crear modelo basado en SVM\n",
    "# Este recibe los hiperparámetros de kernel, c (regularización) y gamma\n",
    "def svm_model(x_train, y_train, x_test, y_test, kernel, c, gamma):\n",
    "    # Registramos la hora de inicio\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Ejecutamos SVM\n",
    "    classifier = svm.SVC(kernel=kernel, C=c, gamma=gamma)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_predict = classifier.predict(x_test)\n",
    "    \n",
    "    # Registramos la hora de finalización\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Calculamos algunas métricas del modelo\n",
    "    model_score = classifier.score(x_test, y_test)\n",
    "    model_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "    model_precision = metrics.precision_score(y_test, y_predict)\n",
    "    model_recall = metrics.recall_score(y_test, y_predict)\n",
    "    \n",
    "    # Creamos un DataFrame con la información a almacenar en csv\n",
    "    data = {'fecha_hora_inicio':[start_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'fecha_hora_fin':[end_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'kernel':[kernel],\n",
    "               'c':[c],\n",
    "               'gamma':[gamma],\n",
    "               'model_score':[model_score],\n",
    "               'model_accuracy':[model_accuracy],\n",
    "               'model_precision':[model_precision],\n",
    "               'model_recall':[model_recall]}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    registrar_experimento(df, 'svm_output.csv')\n",
    "    joblib.dump(classifier, 'svm_clasificador.pkl')\n",
    "    return df\n",
    "    \n",
    "    # ESTRUCTURA DEL DF DE RESULTADO \n",
    "    # fecha_hora_inicio: Fecha y hora de inicio\n",
    "    # fecha_hora_fin: Fecha y hora de finalizacion\n",
    "    # kernel: Kernel usado\n",
    "    # c: Valor de regularización (c)\n",
    "    # gamma: Valor de gamma\n",
    "    # model_score: Punteo del SVM\n",
    "    # model_accuracy: Accuracy del SVM\n",
    "    # model_precision: Precision del SVM\n",
    "    # model_recall: Recall del SVM    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-893ccb658c9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mmodel_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mds_resultados_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_resultados_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_svm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-c5818972b05e>\u001b[0m in \u001b[0;36msvm_model\u001b[1;34m(x_train, y_train, x_test, y_test, kernel, c, gamma)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Ejecutamos SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Aplicamos SVM a nuestros datos de entrenamiento y validación\n",
    "# Variamos los valores de C y gamma para obtener distintos resultados y de éstos se elegirá el mejor\n",
    "c = 30\n",
    "ds_resultados_svm = pd.DataFrame()\n",
    "\n",
    "while c > 0:\n",
    "    gamma = 30\n",
    "    while gamma > 0:\n",
    "        model_svm = svm_model(x_train, y_train, x_validation, y_validation, 'linear', c, gamma)\n",
    "        ds_resultados_svm = ds_resultados_svm.append(model_svm)\n",
    "        \n",
    "        if gamma > 10:\n",
    "            gamma = float('%.4f'%(gamma - 10))\n",
    "        elif gamma > 1:\n",
    "            gamma = float('%.4f'%(gamma - 1))\n",
    "        #elif gamma > 0.1:\n",
    "            #gamma = float('%.4f'%(gamma - 0.1))\n",
    "        #elif gamma > 0.01:\n",
    "            #gamma = float('%.4f'%(gamma - 0.01))\n",
    "        #elif gamma > 0.001:\n",
    "            #gamma = float('%.4f'%(gamma - 0.001))\n",
    "        else:\n",
    "            gamma = 0\n",
    "    if c > 10:\n",
    "        c = float('%.4f'%(c - 10))\n",
    "    elif c > 1:\n",
    "        c = float('%.4f'%(c - 1))\n",
    "    #elif c > 0.1:\n",
    "        #c = float('%.4f'%(c - 0.1))\n",
    "    #elif c > 0.01:\n",
    "        #c = float('%.4f'%(c - 0.01))\n",
    "    #elif c > 0.001:\n",
    "        #c = float('%.4f'%(c - 0.001))\n",
    "    else:\n",
    "        c = 0\n",
    "\n",
    "# Imprimimos los resultados obtenidos\n",
    "print(ds_resultados_svm)\n",
    "\n",
    "# Imprimimos el modelo con el mayor score\n",
    "idxmax = ds_resultados_svm['model_score'].idxmax()\n",
    "print(ds_resultados_svm.loc[idxmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/tree.html\n",
    "# Definimos función para crear modelo basado en Decision Tree\n",
    "def decision_tree_model(x_train, y_train, x_test, y_test):\n",
    "    # Registramos la hora de inicio\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Ejecutamos Decision Tree\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_predict = classifier.predict(x_test)\n",
    "    \n",
    "    # Registramos la hora de finalización\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Calculamos algunas métricas del modelo\n",
    "    model_score = classifier.score(x_test, y_test)\n",
    "    model_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "    model_precision = metrics.precision_score(y_test, y_predict)\n",
    "    model_recall = metrics.recall_score(y_test, y_predict)\n",
    "    \n",
    "    # Creamos un DataFrame con la información a almacenar en csv\n",
    "    data = {'fecha_hora_inicio':[start_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'fecha_hora_fin':[end_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'model_score':[model_score],\n",
    "               'model_accuracy':[model_accuracy],\n",
    "               'model_precision':[model_precision],\n",
    "               'model_recall':[model_recall]}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    registrar_experimento(df, 'decision_tree_output.csv')\n",
    "    joblib.dump(classifier, 'decision_tree_clasificador.pkl')\n",
    "    return df\n",
    "    \n",
    "    # ESTRUCTURA DEL DF DE RESULTADO \n",
    "    # fecha_hora_inicio: Fecha y hora de inicio\n",
    "    # fecha_hora_fin: Fecha y hora de finalizacion\n",
    "    # model_score: Punteo del Decision Tree\n",
    "    # model_accuracy: Accuracy del Decision Tree\n",
    "    # model_precision: Precision del Decision Tree\n",
    "    # model_recall: Recall del Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fecha_hora_inicio        fecha_hora_fin  model_score  model_accuracy  \\\n",
      "0  13/07/2019, 15:10:39  13/07/2019, 15:10:39     0.719626        0.719626   \n",
      "\n",
      "   model_precision  model_recall  \n",
      "0         0.612903      0.513514  \n"
     ]
    }
   ],
   "source": [
    "# Aplicamos Decision Tree a nuestros datos de entrenamiento y validación\n",
    "ds_resultados_decision_tree = decision_tree_model(x_train, y_train, x_validation, y_validation)\n",
    "print(ds_resultados_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_summary(x_train, y_train, y_column_name, y_value):\n",
    "    # Calculamos los valores para Y\n",
    "    summary = pd.DataFrame(columns=['Column', 'Item', 'Probability'])\n",
    "    summary_conditional = pd.DataFrame(columns=['Column', 'YItem', 'Item', 'Probability'])\n",
    "    y_total_rows = y_train.shape[0]\n",
    "    y_column_distinct_values = y_train.unique().tolist()\n",
    "    for column_value in y_column_distinct_values:\n",
    "        value_probability = y_train[y_train == column_value].count() / y_total_rows\n",
    "        summary = summary.append({'Column': y_column_name, 'Item': column_value, 'Probability': value_probability}, ignore_index=True)\n",
    "    \n",
    "    # Construimos nuestras tablas de probabilidad utilizando la data en x_train y y_train\n",
    "    x_total_rows = x_train.shape[0]\n",
    "    x_colnames = x_train.columns\n",
    "    for column in x_colnames:\n",
    "        column_distinct_values = x_train[column].unique().tolist()\n",
    "        for column_value in column_distinct_values:\n",
    "            column_values = x_train[column]\n",
    "            value_probability = column_values[column_values == column_value].count() / x_total_rows\n",
    "            summary = summary.append({'Column': column, 'Item': column_value, 'Probability': value_probability}, ignore_index=True)\n",
    "            for y_column_value in y_column_distinct_values:\n",
    "                combined_columns = pd.DataFrame(columns=['YColumn', 'XColumn'])\n",
    "                combined_columns['YColumn'] = y_train\n",
    "                combined_columns['XColumn'] = column_values\n",
    "                combined_columns = combined_columns[combined_columns['YColumn']==y_column_value]\n",
    "                y_column_value_count = len(combined_columns)\n",
    "                combined_columns = combined_columns[combined_columns['XColumn']==column_value]\n",
    "                combined_columns_count = len(combined_columns)\n",
    "                summary_conditional = summary_conditional.append({'Column': column, 'YItem': y_column_value, 'Item': column_value, 'Probability': combined_columns_count/y_column_value_count}, ignore_index=True)\n",
    "    \n",
    "    # Calculamos nuestra tabla final\n",
    "    numerador = 1\n",
    "    denominador = 1\n",
    "    \n",
    "    # Calculamos el total de registros de y_train\n",
    "    y_train_count = len(y_train)\n",
    "    y_value_count = len(y_train[y_train==y_value])\n",
    "    y_value_probability = y_value_count / y_train_count\n",
    "    \n",
    "    x_colnames = x_train.columns\n",
    "    naive_bayes_summary = pd.DataFrame(columns=['Column', 'Item', 'Probability'])\n",
    "    for column in x_colnames:\n",
    "        column_distinct_values = x_train[column].unique().tolist()\n",
    "        for column_value in column_distinct_values:\n",
    "            conditional_probability = summary_conditional[(summary_conditional['YItem']==y_value) \n",
    "                                                          & (summary_conditional['Column']==column)\n",
    "                                                         & (summary_conditional['Item']==column_value)]\n",
    "            \n",
    "            probability_x = summary[(summary['Column']==column) \n",
    "                                    & (summary['Item']==column_value)]\n",
    "            \n",
    "            numerador = conditional_probability['Probability'].iloc[0] * y_value_probability\n",
    "            denominador = probability_x['Probability'].iloc[0]\n",
    "            naive_bayes_summary = naive_bayes_summary.append({'Column': column, 'Item': column_value, 'Probability': numerador/denominador}, ignore_index=True)\n",
    "    \n",
    "    return naive_bayes_summary\n",
    "\n",
    "#def naive_bayes_model(x_train, y_train, x_test, y_test, y_column_name, y_value):\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Column Item  Probability\n",
      "0    passenger_sex    0     0.755869\n",
      "1    passenger_sex    1     0.181122\n",
      "2         Embarked    0     0.575221\n",
      "3         Embarked    2     0.358491\n",
      "4         Embarked    3     0.334096\n",
      "5         Embarked    1     1.000000\n",
      "6  passenger_class    2     0.629870\n",
      "7  passenger_class    0     0.236527\n",
      "8  passenger_class    1     0.478632\n"
     ]
    }
   ],
   "source": [
    "# Se envía el valor de 1 en el valor de y que se quiere predecir, es decir, la probabilidad de que y tome ese valor\n",
    "# En este caso, y = 1, que implica que la persona se salva\n",
    "# Tomaremos en cuenta unicamente las variables categoricas\n",
    "x_label_categoricas = np.delete(categorical_features, np.where(categorical_features == predict_feature), axis=0)\n",
    "x_train_categoricas = x_train[x_label_categoricas]\n",
    "summary = naive_bayes_summary(x_train_categoricas, y_train, predict_feature, 1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Column Item  Probability\n",
      "0    passenger_sex    0     0.755869\n",
      "1    passenger_sex    1     0.181122\n",
      "2         Embarked    0     0.575221\n",
      "3         Embarked    2     0.358491\n",
      "4         Embarked    3     0.334096\n",
      "5         Embarked    1     1.000000\n",
      "6  passenger_class    2     0.629870\n",
      "7  passenger_class    0     0.236527\n",
      "8  passenger_class    1     0.478632\n",
      "Age\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Age\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Age\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "SibSp\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "SibSp\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "SibSp\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Parch\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Parch\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Parch\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Fare\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Fare\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Fare\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Embarked\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Embarked\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "Embarked\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_class\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_class\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_class\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_sex\n",
      "passenger_sex\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_sex\n",
      "Embarked\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n",
      "passenger_sex\n",
      "passenger_class\n",
      "Empty DataFrame\n",
      "Columns: [Column, Item, Probability]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la probabilidad de sobrevivir para los items en el set de validación\n",
    "print(summary)\n",
    "for row in x_validation:\n",
    "    probabilidad = 1;\n",
    "    for column in row.columns:\n",
    "        probabilidad = summary[(summary['Column']==column) \n",
    "                               & (summary['Item']==column[])]\n",
    "        print(probabilidad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
