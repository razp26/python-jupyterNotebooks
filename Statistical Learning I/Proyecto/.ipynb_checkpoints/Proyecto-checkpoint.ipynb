{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Entrenamiento, validación y selección\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chofo\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerias que utilizaremos a lo largo del proyecto\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-val-test-split\n",
    "\n",
    "El primer es dar un vistazo a los datos con los que se cuenta. Iniciamos validando el tipo de dato de cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Iniciamos leyendo el archivo de entrada\n",
    "dataset = pd.read_csv(\"data_titanic_proyecto.csv\")\n",
    "\n",
    "# Contamos el número de regisitros y el numero de columnas que tiene el dataset\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId             int64\n",
       "Name                   object\n",
       "Age                   float64\n",
       "SibSp                   int64\n",
       "Parch                   int64\n",
       "Ticket                 object\n",
       "Fare                  float64\n",
       "Cabin                  object\n",
       "Embarked               object\n",
       "passenger_class        object\n",
       "passenger_sex          object\n",
       "passenger_survived     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos el tipo de dato que tiene cada columna en el dataset\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Porcentaje NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>19.865320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>77.104377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0.224467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_class</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_sex</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_survived</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Porcentaje NaN\n",
       "PassengerId               0.000000\n",
       "Name                      0.000000\n",
       "Age                      19.865320\n",
       "SibSp                     0.000000\n",
       "Parch                     0.000000\n",
       "Ticket                    0.000000\n",
       "Fare                      0.000000\n",
       "Cabin                    77.104377\n",
       "Embarked                  0.224467\n",
       "passenger_class           0.000000\n",
       "passenger_sex             0.000000\n",
       "passenger_survived        0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el portancaje de NaN en nuestro dataset\n",
    "pd.DataFrame({'Porcentaje NaN': dataset.isnull().sum() * 100 / len(dataset)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con base al análisis anterior, procedemos a descartar los features de: PassengerId, Name, Ticket y Cabin.\n",
    "\n",
    "La columna 'Cabin' se descarta debido a que posee un 77% de datos NaN los cuales representan una gran incertidumbre. A continuación procedemos a definir 3 variables, una que contendrá las features que no se utilizarán, otra que contendrá las features categoricas y la última el nombre del feature a predecir, en este caso será passenger_survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(ds, useless_features, categorical_features):\n",
    "    \n",
    "    # Eliminamos las features que no se utilizaran del dataset\n",
    "    ds = ds.drop(useless_features, axis=1)\n",
    "    \n",
    "    # Normalizamos las variables que no son categoricas\n",
    "    # Sustituiremos los valores NaN por el valor de la media\n",
    "    imputer = Imputer(strategy=\"median\")\n",
    "    ds_numeric_features = ds.drop(categorical_features, axis=1)\n",
    "    imputer.fit(ds_numeric_features)\n",
    "    ds_imputer = imputer.transform(ds_numeric_features)\n",
    "    ds_numeric_features = pd.DataFrame(ds_imputer, columns=ds_numeric_features.columns)\n",
    "    \n",
    "    # Normalizamos las variables categoricas\n",
    "    ds_categorical_features = ds.drop(ds_numeric_features.columns, axis=1)\n",
    "    ds_categorical_features_encoded = pd.DataFrame()\n",
    "    \n",
    "    # Sustituiremos los valores NaN por 'NaN'\n",
    "    ds_categorical_features = ds_categorical_features.replace(np.nan, 'NaN', regex=True)\n",
    "    \n",
    "    # Usamos LabelEncoder para convertir nuestros valores categoricos a numericos\n",
    "    ds_categorical_features = ds_categorical_features.apply(LabelEncoder().fit_transform)\n",
    "    ds_categorical_features = pd.DataFrame(ds_categorical_features, columns= ds_categorical_features.columns)\n",
    "    \n",
    "    # Concatenamos nuestros dos DataFrames\n",
    "    ds[categorical_features] = ds_categorical_features[categorical_features].values\n",
    "    ds[ds_numeric_features.columns] = ds_numeric_features.values\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age  SibSp  Parch      Fare  Embarked  passenger_class  passenger_sex  \\\n",
      "0    22.0    1.0    0.0    7.2500         3                0              1   \n",
      "1    38.0    1.0    0.0   71.2833         0                2              0   \n",
      "2    26.0    0.0    0.0    7.9250         3                0              0   \n",
      "3    35.0    1.0    0.0   53.1000         3                2              0   \n",
      "4    35.0    0.0    0.0    8.0500         3                0              1   \n",
      "5    28.0    0.0    0.0    8.4583         2                0              1   \n",
      "6    54.0    0.0    0.0   51.8625         3                2              1   \n",
      "7     2.0    3.0    1.0   21.0750         3                0              1   \n",
      "8    27.0    0.0    2.0   11.1333         3                0              0   \n",
      "9    14.0    1.0    0.0   30.0708         0                1              0   \n",
      "10    4.0    1.0    1.0   16.7000         3                0              0   \n",
      "11   58.0    0.0    0.0   26.5500         3                2              0   \n",
      "12   20.0    0.0    0.0    8.0500         3                0              1   \n",
      "13   39.0    1.0    5.0   31.2750         3                0              1   \n",
      "14   14.0    0.0    0.0    7.8542         3                0              0   \n",
      "15   55.0    0.0    0.0   16.0000         3                1              0   \n",
      "16    2.0    4.0    1.0   29.1250         2                0              1   \n",
      "17   28.0    0.0    0.0   13.0000         3                1              1   \n",
      "18   31.0    1.0    0.0   18.0000         3                0              0   \n",
      "19   28.0    0.0    0.0    7.2250         0                0              0   \n",
      "20   35.0    0.0    0.0   26.0000         3                1              1   \n",
      "21   34.0    0.0    0.0   13.0000         3                1              1   \n",
      "22   15.0    0.0    0.0    8.0292         2                0              0   \n",
      "23   28.0    0.0    0.0   35.5000         3                2              1   \n",
      "24    8.0    3.0    1.0   21.0750         3                0              0   \n",
      "25   38.0    1.0    5.0   31.3875         3                0              0   \n",
      "26   28.0    0.0    0.0    7.2250         0                0              1   \n",
      "27   19.0    3.0    2.0  263.0000         3                2              1   \n",
      "28   28.0    0.0    0.0    7.8792         2                0              0   \n",
      "29   28.0    0.0    0.0    7.8958         3                0              1   \n",
      "..    ...    ...    ...       ...       ...              ...            ...   \n",
      "861  21.0    1.0    0.0   11.5000         3                1              1   \n",
      "862  48.0    0.0    0.0   25.9292         3                2              0   \n",
      "863  28.0    8.0    2.0   69.5500         3                0              0   \n",
      "864  24.0    0.0    0.0   13.0000         3                1              1   \n",
      "865  42.0    0.0    0.0   13.0000         3                1              0   \n",
      "866  27.0    1.0    0.0   13.8583         0                1              0   \n",
      "867  31.0    0.0    0.0   50.4958         3                2              1   \n",
      "868  28.0    0.0    0.0    9.5000         3                0              1   \n",
      "869   4.0    1.0    1.0   11.1333         3                0              1   \n",
      "870  26.0    0.0    0.0    7.8958         3                0              1   \n",
      "871  47.0    1.0    1.0   52.5542         3                2              0   \n",
      "872  33.0    0.0    0.0    5.0000         3                2              1   \n",
      "873  47.0    0.0    0.0    9.0000         3                0              1   \n",
      "874  28.0    1.0    0.0   24.0000         0                1              0   \n",
      "875  15.0    0.0    0.0    7.2250         0                0              0   \n",
      "876  20.0    0.0    0.0    9.8458         3                0              1   \n",
      "877  19.0    0.0    0.0    7.8958         3                0              1   \n",
      "878  28.0    0.0    0.0    7.8958         3                0              1   \n",
      "879  56.0    0.0    1.0   83.1583         0                2              0   \n",
      "880  25.0    0.0    1.0   26.0000         3                1              0   \n",
      "881  33.0    0.0    0.0    7.8958         3                0              1   \n",
      "882  22.0    0.0    0.0   10.5167         3                0              0   \n",
      "883  28.0    0.0    0.0   10.5000         3                1              1   \n",
      "884  25.0    0.0    0.0    7.0500         3                0              1   \n",
      "885  39.0    0.0    5.0   29.1250         2                0              0   \n",
      "886  27.0    0.0    0.0   13.0000         3                1              1   \n",
      "887  19.0    0.0    0.0   30.0000         3                2              0   \n",
      "888  28.0    1.0    2.0   23.4500         3                0              0   \n",
      "889  26.0    0.0    0.0   30.0000         0                2              1   \n",
      "890  32.0    0.0    0.0    7.7500         2                0              1   \n",
      "\n",
      "     passenger_survived  \n",
      "0                     0  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                     1  \n",
      "4                     0  \n",
      "5                     0  \n",
      "6                     0  \n",
      "7                     0  \n",
      "8                     1  \n",
      "9                     1  \n",
      "10                    1  \n",
      "11                    1  \n",
      "12                    0  \n",
      "13                    0  \n",
      "14                    0  \n",
      "15                    1  \n",
      "16                    0  \n",
      "17                    1  \n",
      "18                    0  \n",
      "19                    1  \n",
      "20                    0  \n",
      "21                    1  \n",
      "22                    1  \n",
      "23                    1  \n",
      "24                    0  \n",
      "25                    1  \n",
      "26                    0  \n",
      "27                    0  \n",
      "28                    1  \n",
      "29                    0  \n",
      "..                  ...  \n",
      "861                   0  \n",
      "862                   1  \n",
      "863                   0  \n",
      "864                   0  \n",
      "865                   1  \n",
      "866                   1  \n",
      "867                   0  \n",
      "868                   0  \n",
      "869                   1  \n",
      "870                   0  \n",
      "871                   1  \n",
      "872                   0  \n",
      "873                   0  \n",
      "874                   1  \n",
      "875                   1  \n",
      "876                   0  \n",
      "877                   0  \n",
      "878                   0  \n",
      "879                   1  \n",
      "880                   1  \n",
      "881                   0  \n",
      "882                   0  \n",
      "883                   0  \n",
      "884                   0  \n",
      "885                   0  \n",
      "886                   0  \n",
      "887                   1  \n",
      "888                   0  \n",
      "889                   1  \n",
      "890                   0  \n",
      "\n",
      "[891 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chofo\\Anaconda3\\envs\\galileo_python\\lib\\site-packages\\sklearn\\utils\\deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos feature engineering para normalizar los datos a una forma que no sea útil\n",
    "useless_features = np.array(['PassengerId','Name','Ticket', 'Cabin'])\n",
    "categorical_features = np.array(['passenger_sex','Embarked','passenger_class','passenger_survived'])\n",
    "normalized_dataset = feature_engineering(dataset, useless_features, categorical_features)\n",
    "\n",
    "print (normalized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego que hemos hecho feature_engineering sobre nuestra data para normalizarla, procedemos a crear nuestros distintos datasets que utilizaremos para entrenar, validar y probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de los datos de entrenamiento:  (605, 7) (605,)\n",
      "Shape de los datos de validación (107, 7) (107,)\n",
      "Shape de los datos de prueba (179, 7) (179,)\n"
     ]
    }
   ],
   "source": [
    "# Hacemos una copia de nuestro dataset normalizado sin incluir la variable a predecir\n",
    "ds_x = normalized_dataset.drop('passenger_survived', axis=1)\n",
    "\n",
    "predict_feature = 'passenger_survived'\n",
    "ds_y = normalized_dataset[predict_feature].copy()\n",
    "\n",
    "# Separamos nuestros datos en datos de prueba y datos de entrenamiento\n",
    "# Utiliazaremos el 80% como datos de entrenamiento\n",
    "x_train, x_test, y_train, y_test = train_test_split(ds_x, ds_y, train_size=0.8)\n",
    "\n",
    "# Utilizamos los datos de entrenamiento para extraer una porción para datos de validación.\n",
    "# Utilizaremos un 20% del dataset de entrenamiento para datos de validación\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.85)\n",
    "\n",
    "print('Shape de los datos de entrenamiento: ', x_train.shape, y_train.shape)\n",
    "print('Shape de los datos de validación', x_validation.shape, y_validation.shape)\n",
    "print('Shape de los datos de prueba', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registro de bitácora\n",
    "\n",
    "Definimos unos métodos que serán de utilidad para registrar la información relevante de cada experimento que se haga con cada uno de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registrar_experimento(df, nombre_archivo):\n",
    "    if not os.path.isfile(nombre_archivo):\n",
    "        # Si el archivo no existe, lo crearmos y agregamos el dataframe con todo y sus encabezados\n",
    "        df.to_csv(nombre_archivo)\n",
    "    else: \n",
    "        # Si el archivo ya existe, agregamos el dataframe sin incluir sus encabezados\n",
    "        df.to_csv(nombre_archivo, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "A lo largo del proyecto se utilizará Ensemble Learning de cuatro distintos modelos obtenidos a partir de:\n",
    "\n",
    "- Árbol de decisión con sklearn\n",
    "- SVM con sklearn\n",
    "- Naive bayes con numpy y/o pandas\n",
    "- Reg. logística binaria(sigmoid) utilizando Tensorflow\n",
    "\n",
    "\n",
    "# Bootstraping\n",
    "\n",
    "A pesar de que en el proyecto no se utiliza muestreo por bootstrap, numpy provee de la función \"numpy.random.sample\" en donde debería colocarse el parámetro replace = True.\n",
    "\n",
    "Básicamente lo que hace es tomar muestras aleatorias del dataset que se tiene. Esto es de gran utilidad para evitar que todos los datos para test, train y validation se encuentren ubicados en las mismas secciones del dataset. Esto hará que cada uno de éstos sets de datos esté conformado de valores aleatorios dentro del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos función para crear modelo basado en SVM\n",
    "# Este recibe los hiperparámetros de kernel, c (regularización) y gamma\n",
    "def svm_model(x_train, y_train, x_test, y_test, kernel, c, gamma):\n",
    "    # Registramos la hora de inicio\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Ejecutamos SVM\n",
    "    classifier = svm.SVC(kernel=kernel, C=c, gamma=gamma)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_predict = classifier.predict(x_test)\n",
    "    \n",
    "    # Registramos la hora de finalización\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Calculamos algunas métricas del modelo\n",
    "    model_score = classifier.score(x_test, y_test)\n",
    "    model_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "    model_precision = metrics.precision_score(y_test, y_predict)\n",
    "    model_recall = metrics.recall_score(y_test, y_predict)\n",
    "    \n",
    "    # Creamos un DataFrame con la información a almacenar en csv\n",
    "    data = {'fecha_hora_inicio':[start_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'fecha_hora_fin':[end_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'kernel':[kernel],\n",
    "               'c':[c],\n",
    "               'gamma':[gamma],\n",
    "               'model_score':[model_score],\n",
    "               'model_accuracy':[model_accuracy],\n",
    "               'model_precision':[model_precision],\n",
    "               'model_recall':[model_recall]}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    registrar_experimento(df, 'svm_output.csv')\n",
    "    joblib.dump(classifier, 'svm_clasificador.pkl')\n",
    "    return df\n",
    "    \n",
    "    # ESTRUCTURA DEL DF DE RESULTADO \n",
    "    # fecha_hora_inicio: Fecha y hora de inicio\n",
    "    # fecha_hora_fin: Fecha y hora de finalizacion\n",
    "    # kernel: Kernel usado\n",
    "    # c: Valor de regularización (c)\n",
    "    # gamma: Valor de gamma\n",
    "    # model_score: Punteo del SVM\n",
    "    # model_accuracy: Accuracy del SVM\n",
    "    # model_precision: Precision del SVM\n",
    "    # model_recall: Recall del SVM    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fecha_hora_inicio        fecha_hora_fin  kernel  c  gamma  model_score  \\\n",
      "0  14/07/2019, 22:37:46  14/07/2019, 22:38:06  linear  1      1     0.813084   \n",
      "\n",
      "   model_accuracy  model_precision  model_recall  \n",
      "0        0.813084         0.806452      0.641026  \n",
      "fecha_hora_inicio    14/07/2019, 22:37:46\n",
      "fecha_hora_fin       14/07/2019, 22:38:06\n",
      "kernel                             linear\n",
      "c                                       1\n",
      "gamma                                   1\n",
      "model_score                      0.813084\n",
      "model_accuracy                   0.813084\n",
      "model_precision                  0.806452\n",
      "model_recall                     0.641026\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos SVM a nuestros datos de entrenamiento y validación\n",
    "# Variamos los valores de C y gamma para obtener distintos resultados y de éstos se elegirá el mejor\n",
    "c = 1\n",
    "ds_resultados_svm = pd.DataFrame()\n",
    "\n",
    "while c > 0:\n",
    "    gamma = 1\n",
    "    while gamma > 0:\n",
    "        model_svm = svm_model(x_train, y_train, x_validation, y_validation, 'linear', c, gamma)\n",
    "        ds_resultados_svm = ds_resultados_svm.append(model_svm)\n",
    "        \n",
    "        if gamma > 10:\n",
    "            gamma = float('%.4f'%(gamma - 10))\n",
    "        elif gamma > 1:\n",
    "            gamma = float('%.4f'%(gamma - 1))\n",
    "        #elif gamma > 0.1:\n",
    "            #gamma = float('%.4f'%(gamma - 0.1))\n",
    "        #elif gamma > 0.01:\n",
    "            #gamma = float('%.4f'%(gamma - 0.01))\n",
    "        #elif gamma > 0.001:\n",
    "            #gamma = float('%.4f'%(gamma - 0.001))\n",
    "        else:\n",
    "            gamma = 0\n",
    "    if c > 10:\n",
    "        c = float('%.4f'%(c - 10))\n",
    "    elif c > 1:\n",
    "        c = float('%.4f'%(c - 1))\n",
    "    #elif c > 0.1:\n",
    "        #c = float('%.4f'%(c - 0.1))\n",
    "    #elif c > 0.01:\n",
    "        #c = float('%.4f'%(c - 0.01))\n",
    "    #elif c > 0.001:\n",
    "        #c = float('%.4f'%(c - 0.001))\n",
    "    else:\n",
    "        c = 0\n",
    "\n",
    "# Imprimimos los resultados obtenidos\n",
    "print(ds_resultados_svm)\n",
    "\n",
    "# Imprimimos el modelo con el mayor score\n",
    "idxmax = ds_resultados_svm['model_score'].idxmax()\n",
    "print(ds_resultados_svm.loc[idxmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/tree.html\n",
    "# Definimos función para crear modelo basado en Decision Tree\n",
    "def decision_tree_model(x_train, y_train, x_test, y_test):\n",
    "    # Registramos la hora de inicio\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Ejecutamos Decision Tree\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_predict = classifier.predict(x_test)\n",
    "    \n",
    "    # Registramos la hora de finalización\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Calculamos algunas métricas del modelo\n",
    "    model_score = classifier.score(x_test, y_test)\n",
    "    model_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "    model_precision = metrics.precision_score(y_test, y_predict)\n",
    "    model_recall = metrics.recall_score(y_test, y_predict)\n",
    "    \n",
    "    # Creamos un DataFrame con la información a almacenar en csv\n",
    "    data = {'fecha_hora_inicio':[start_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'fecha_hora_fin':[end_time.strftime(\"%d/%m/%Y, %H:%M:%S\")],\n",
    "               'model_score':[model_score],\n",
    "               'model_accuracy':[model_accuracy],\n",
    "               'model_precision':[model_precision],\n",
    "               'model_recall':[model_recall]}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    registrar_experimento(df, 'decision_tree_output.csv')\n",
    "    joblib.dump(classifier, 'decision_tree_clasificador.pkl')\n",
    "    return df\n",
    "    \n",
    "    # ESTRUCTURA DEL DF DE RESULTADO \n",
    "    # fecha_hora_inicio: Fecha y hora de inicio\n",
    "    # fecha_hora_fin: Fecha y hora de finalizacion\n",
    "    # model_score: Punteo del Decision Tree\n",
    "    # model_accuracy: Accuracy del Decision Tree\n",
    "    # model_precision: Precision del Decision Tree\n",
    "    # model_recall: Recall del Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fecha_hora_inicio        fecha_hora_fin  model_score  model_accuracy  \\\n",
      "0  13/07/2019, 15:10:39  13/07/2019, 15:10:39     0.719626        0.719626   \n",
      "\n",
      "   model_precision  model_recall  \n",
      "0         0.612903      0.513514  \n"
     ]
    }
   ],
   "source": [
    "# Aplicamos Decision Tree a nuestros datos de entrenamiento y validación\n",
    "ds_resultados_decision_tree = decision_tree_model(x_train, y_train, x_validation, y_validation)\n",
    "print(ds_resultados_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_summary(x_train, y_train, y_column_name, y_value):\n",
    "    # Calculamos los valores para Y\n",
    "    summary = pd.DataFrame(columns=['Column', 'Item', 'Probability'])\n",
    "    summary_conditional = pd.DataFrame(columns=['Column', 'YItem', 'Item', 'Probability'])\n",
    "    y_total_rows = y_train.shape[0]\n",
    "    y_column_distinct_values = y_train.unique().tolist()\n",
    "    for column_value in y_column_distinct_values:\n",
    "        value_probability = y_train[y_train == column_value].count() / y_total_rows\n",
    "        summary = summary.append({'Column': y_column_name, 'Item': column_value, 'Probability': value_probability}, ignore_index=True)\n",
    "    \n",
    "    # Construimos nuestras tablas de probabilidad utilizando la data en x_train y y_train\n",
    "    x_total_rows = x_train.shape[0]\n",
    "    x_colnames = x_train.columns\n",
    "    for column in x_colnames:\n",
    "        column_distinct_values = x_train[column].unique().tolist()\n",
    "        for column_value in column_distinct_values:\n",
    "            column_values = x_train[column]\n",
    "            value_probability = column_values[column_values == column_value].count() / x_total_rows\n",
    "            summary = summary.append({'Column': column, 'Item': column_value, 'Probability': value_probability}, ignore_index=True)\n",
    "            for y_column_value in y_column_distinct_values:\n",
    "                combined_columns = pd.DataFrame(columns=['YColumn', 'XColumn'])\n",
    "                combined_columns['YColumn'] = y_train\n",
    "                combined_columns['XColumn'] = column_values\n",
    "                combined_columns = combined_columns[combined_columns['YColumn']==y_column_value]\n",
    "                y_column_value_count = len(combined_columns)\n",
    "                combined_columns = combined_columns[combined_columns['XColumn']==column_value]\n",
    "                combined_columns_count = len(combined_columns)\n",
    "                summary_conditional = summary_conditional.append({'Column': column, 'YItem': y_column_value, 'Item': column_value, 'Probability': combined_columns_count/y_column_value_count}, ignore_index=True)\n",
    "    \n",
    "    # Calculamos nuestra tabla final\n",
    "    numerador = 1\n",
    "    denominador = 1\n",
    "    \n",
    "    # Calculamos el total de registros de y_train\n",
    "    y_train_count = len(y_train)\n",
    "    y_value_count = len(y_train[y_train==y_value])\n",
    "    y_value_probability = y_value_count / y_train_count\n",
    "    \n",
    "    x_colnames = x_train.columns\n",
    "    naive_bayes_summary = pd.DataFrame(columns=['Column', 'Item', 'Probability'])\n",
    "    for column in x_colnames:\n",
    "        column_distinct_values = x_train[column].unique().tolist()\n",
    "        for column_value in column_distinct_values:\n",
    "            conditional_probability = summary_conditional[(summary_conditional['YItem']==y_value) \n",
    "                                                          & (summary_conditional['Column']==column)\n",
    "                                                         & (summary_conditional['Item']==column_value)]\n",
    "            \n",
    "            probability_x = summary[(summary['Column']==column) \n",
    "                                    & (summary['Item']==column_value)]\n",
    "            \n",
    "            numerador = conditional_probability['Probability'].iloc[0] * y_value_probability\n",
    "            denominador = probability_x['Probability'].iloc[0]\n",
    "            naive_bayes_summary = naive_bayes_summary.append({'Column': column, 'Item': column_value, 'Probability': numerador/denominador}, ignore_index=True)\n",
    "    \n",
    "    return naive_bayes_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Column Item  Probability\n",
      "0    passenger_sex    0     0.755869\n",
      "1    passenger_sex    1     0.181122\n",
      "2         Embarked    0     0.575221\n",
      "3         Embarked    2     0.358491\n",
      "4         Embarked    3     0.334096\n",
      "5         Embarked    1     1.000000\n",
      "6  passenger_class    2     0.629870\n",
      "7  passenger_class    0     0.236527\n",
      "8  passenger_class    1     0.478632\n"
     ]
    }
   ],
   "source": [
    "# Se envía el valor de 1 en el valor de y que se quiere predecir, es decir, la probabilidad de que y tome ese valor\n",
    "# En este caso, y = 1, que implica que la persona se salva\n",
    "# Tomaremos en cuenta unicamente las variables categoricas\n",
    "x_label_categoricas = np.delete(categorical_features, np.where(categorical_features == predict_feature), axis=0)\n",
    "x_train_categoricas = x_train[x_label_categoricas]\n",
    "summary = naive_bayes_summary(x_train_categoricas, y_train, predict_feature, 1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.15906282993013907\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.06409211871234942\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.015357857638592667\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2081054006767581\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 2 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.17067700657921228\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.06409211871234942\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2738622671939387\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.015357857638592667\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.15906282993013907\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2738622671939387\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2738622671939387\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.15906282993013907\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.015357857638592667\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.06409211871234942\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.15906282993013907\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.06409211871234942\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.015357857638592667\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.031077936738314105\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2738622671939387\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.02464267329527328\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.15906282993013907\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.059730797855734055\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 0 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0656233215042137\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 0 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.2081054006767581\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 2]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.03811489379251955\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 2 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.015357857638592667\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[0 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.12087037142633884\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 1]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.0289631548212784\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n",
      "Características: \n",
      "Index(['passenger_sex', 'Embarked', 'passenger_class'], dtype='object')\n",
      "Valores\n",
      "[1 3 0]\n",
      "Probabilidad de  passenger_survived  =  1\n",
      "0.014312790847576787\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la probabilidad de sobrevivir para los items en el set de validación\n",
    "x_validation_categoricas = x_validation[x_label_categoricas]\n",
    "for index, row in x_validation_categoricas.iterrows():\n",
    "    probabilidad = 1;\n",
    "    for column in row.index:\n",
    "        probabilidad_column = summary[(summary['Column']==column) \n",
    "                               & (summary['Item']==row[column])]\n",
    "        probabilidad = probabilidad * probabilidad_column['Probability'].values[0]\n",
    "    print('Características: ')\n",
    "    print(row.index)\n",
    "    print('Valores')\n",
    "    print(row.values)\n",
    "    print('Probabilidad de ',predict_feature, ' = ',1)\n",
    "    print(probabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresion_logistica(x, y, labels, epochs, batch_size, lr, imprimir_error_cada):\n",
    "    \n",
    "    # Preparamos los valores de x a como los utilizaremos\n",
    "    # Convertimos las dimensiones 28 X 28 en una dimension de 784\n",
    "    x = np.reshape(x, x.shape[:-2] + (-1,))\n",
    "    \n",
    "    # Calculamos algunos valores a utilizar en la creación de los placeholders y variables\n",
    "    n_parametros = np.prod(x[:1:2].shape)\n",
    "    n_categorias = len(labels)\n",
    "    n_items = x.shape[0]\n",
    "    \n",
    "    # Utilizamos one hot encoding para definir los valores de los labels y crear un arreglo con los valores del one hot encoding\n",
    "    labels_values = np.array(list(labels.values()))\n",
    "    sklb = sklearn.preprocessing.LabelBinarizer()\n",
    "    sklb.fit(labels_values)\n",
    "    y = sklb.transform(y)\n",
    "    \n",
    "    # Creamos el grafo que utilizaremos\n",
    "    grafo = tf.Graph()\n",
    "    with grafo.as_default():\n",
    "        \n",
    "        # Definimos nuestros placeholders\n",
    "        x_ = tf.placeholder(tf.float32, [None,n_parametros], name=\"x\")\n",
    "        y_ = tf.placeholder(tf.float32, [n_categorias, None], name=\"y\")\n",
    "        lr_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "        \n",
    "        # Definimos nuestras variables\n",
    "        w = tf.Variable(np.zeros([n_parametros,n_categorias]), name=\"w\", dtype= tf.float32) \n",
    "        b = tf.Variable(np.ones([1, n_categorias]), name=\"b\", dtype=tf.float32)\n",
    "        \n",
    "        # Función para calcular el valor de la hipotesis y\n",
    "        with tf.name_scope(\"hipotesis\"):\n",
    "            yhat = tf.nn.softmax(tf.matmul(x_, w, name=\"matmul_xw\") + b, name=\"yhat\")\n",
    "            \n",
    "        # Función de costo con entropía cruzada\n",
    "        with tf.name_scope(\"costo_entropia_cruzada\"):\n",
    "            cross_entropy = -1 * tf.reduce_mean(\n",
    "                tf.matmul(y_, tf.log(yhat, name=\"log_yhat\"), name=\"matmul_y_logyhat\") \n",
    "                + tf.matmul((1 - y_),tf.log(1 - yhat, name=\"log_uno_menos_yhat\"), name=\"matmul_uno_menos_y_por_log_uno_menos_yhat\")\n",
    "                , reduction_indices=[1], name=\"entropia_cruzada\")\n",
    "            #cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y_, logits = yhat)\n",
    "            \n",
    "        # Definimos el optimizador gradient descent\n",
    "        with tf.name_scope(\"optimizador_gradient_descent\"):\n",
    "            optimizador = tf.train.GradientDescentOptimizer(lr_).minimize(cross_entropy) \n",
    "            \n",
    "        # Calculamos el valor correcto de la predicción\n",
    "        with tf.name_scope(\"prediccion_correcta\"):\n",
    "            prediccion_correcta = tf.equal(tf.argmax(y_,axis=0), tf.argmax(yhat,axis=1), name=\"prediccion_correcta\")\n",
    "            \n",
    "        # Calculamos que tan acertado ha estado el modelo\n",
    "        with tf.name_scope(\"exactitud\"):\n",
    "            exactitud = tf.reduce_mean(tf.cast(prediccion_correcta, tf.float32), name=\"exactitud\")\n",
    "        \n",
    "        # Inicializamos las variables globales\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Inicializamos el grafo\n",
    "        with tf.Session(graph = grafo) as session:\n",
    "            \n",
    "            # Creamos un writer para nuestros logs en tensorboard\n",
    "            writer = tf.summary.FileWriter('./grafos/lr=' + str(lr), session.graph)\n",
    "            \n",
    "            # Inicializamos las variables en la sesión\n",
    "            session.run(init)\n",
    "            \n",
    "            # Iteramos el número de veces que indica el parámetro \"epochs\"\n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                # Iteramos de acuerdo al numero de mini batches a utilizar\n",
    "                for i in range(0, n_items, batch_size):\n",
    "                    x_batch = x[i: i + batch_size,:]\n",
    "                    y_batch = y[i: i + batch_size]\n",
    "                    \n",
    "                    # y_batch tiene la forma [batch_size, n_parametros]\n",
    "                    # cambiamos y_batch a la forma [n_parametros, batch_size]\n",
    "                    y_batch = np.transpose(y_batch)\n",
    "\n",
    "                    # Ejecutamos el grafo\n",
    "                    session.run(optimizador, feed_dict={x_:x_batch, y_:y_batch, lr_:lr})\n",
    "                    \n",
    "                    #if (epoch + 1) % imprimir_error_cada == 0:\n",
    "                    #    costo = session.run(cross_entropy, feed_dict={x_:x_batch, y_:y_batch, lr_:lr})\n",
    "                    #    print('Iteración: ' + str(epoch + 1))\n",
    "                    #    print('Entropía cruzada: ')\n",
    "                    #    print(costo)\n",
    "                \n",
    "                if (epoch + 1) % imprimir_error_cada == 0:\n",
    "                    y_t = np.transpose(y)\n",
    "                    resultado_exactitud = session.run(exactitud, feed_dict={x_:x, y_:y_t})\n",
    "                    costo = session.run(cross_entropy, feed_dict={x_:x, y_:y_t})\n",
    "                    print('epoch: ', epoch + 1)\n",
    "                    print('Entropía cruzada: ', costo)\n",
    "                    print('exactitud: ', resultado_exactitud)\n",
    "            \n",
    "                y_t = np.transpose(y)\n",
    "                costos = session.run(cross_entropy, feed_dict={x_:x, y_:y_t})\n",
    "                weights = session.run(w, feed_dict={x_:x, y_:y_t})\n",
    "                bias = session.run(b, feed_dict={x_:x, y_:y_t})\n",
    "            \n",
    "            \n",
    "            print('RESULTADOS FINALES')\n",
    "            print(costos)\n",
    "            print(weights)\n",
    "            print(bias)\n",
    "            \n",
    "            # Cerramos el writer\n",
    "            writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
